{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data & Preprocessing\n",
    "\n",
    "### We can do a combination of the following for data normalization: \n",
    "1. Convert all entities to upper or lower case, and remove whitespace\n",
    "2. Run a spell checker to remove known typographical errors \n",
    "3. Replace nicknames, and expand abbreviations \n",
    "4. Perform looksups in lexicons \n",
    "5. Tokenize, Stem, or Lemmatize words \n",
    "\n",
    "### We can do a combination of the following for missing values: \n",
    "1. Set to Nan, Null, or remove \n",
    "2. Missing entries can also be filled by aggregating other fields or taking means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "entities = pd.read_csv(\"/Users/Rong/Documents/USF/EntityResolution/Model1/named_resolution.csv\")\n",
    "articles = pd.read_json(\"/Users/Rong/Documents/USF/EntityResolution/Model1/articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id         name      type  paragraph\n",
      "0      331842     Broadway  LOCATION          1\n",
      "1      331842  Daniel Fish    PERSON          1\n",
      "2      331842      Rodgers    PERSON          1\n",
      "3      331842  Hammerstein    PERSON          1\n",
      "4      331842     Oklahoma  LOCATION          1\n"
     ]
    }
   ],
   "source": [
    "print(entities.head()[['article_id','name','type','paragraph']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    article_id                  name    type  paragraph\n",
      "1       331842           Daniel Fish  PERSON          1\n",
      "2       331842               Rodgers  PERSON          1\n",
      "3       331842           Hammerstein  PERSON          1\n",
      "6       331842          Damon Daunno  PERSON          3\n",
      "9       331842                  Fish  PERSON          6\n",
      "11      331842  Oscar Hammerstein II  PERSON          7\n",
      "12      331842                  Fish  PERSON          8\n",
      "13      331842           Trevor Nunn  PERSON          8\n",
      "14      331842         Susan Stroman  PERSON          8\n",
      "17      331842            Lynn Riggs  PERSON          9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1             DANIELFISH\n",
       "2                RODGERS\n",
       "3            HAMMERSTEIN\n",
       "6            DAMONDAUNNO\n",
       "9                   FISH\n",
       "11    OSCARHAMMERSTEINII\n",
       "12                  FISH\n",
       "13            TREVORNUNN\n",
       "14          SUSANSTROMAN\n",
       "17             LYNNRIGGS\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(articles.head(5)[['content', 'id']])\n",
    "\n",
    "# Get all entities that are people only \n",
    "entity_people = entities[entities['type'] == 'PERSON']\n",
    "print(entity_people.head(10)[['article_id','name','type','paragraph']])\n",
    "\n",
    "# Capitalise all names and remove spaces and non alphabetical characters\n",
    "entity_people_names_CAPITALS = entity_people['name'].str.upper().str.replace('\\W+', '')\n",
    "entity_people_names_CAPITALS.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "6     False\n",
       "9     False\n",
       "11    False\n",
       "12     True\n",
       "13    False\n",
       "14    False\n",
       "17    False\n",
       "18    False\n",
       "19    False\n",
       "20    False\n",
       "21    False\n",
       "22    False\n",
       "23    False\n",
       "24    False\n",
       "25    False\n",
       "26    False\n",
       "27    False\n",
       "Name: name, dtype: bool"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_people_names_CAPITALS.duplicated().head(20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to obtain distributed representations of words \n",
    "\n",
    "There are several methods to compute the DRs of words: \n",
    "1. word2Vec https://github.com/maxoodf/word2vec\n",
    "2. GloVe https://nlp.stanford.edu/projects/glove/\n",
    "3. fastText https://fasttext.cc/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Clustering entities with GloVe\n",
    "\n",
    "#### GloVe = Global Vectors for Word Representation\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Inputs: Entity 1, Context String Entity 1, Entity 2, Context String Entity 2 \n",
    "#### Outputs: True/False \n",
    "\n",
    "\n",
    "#### Preprocessing \n",
    "Convert all Entities into lower or upper case format\n",
    "Select only entities that are categorised as 'People'\n",
    "\n",
    "#### Clustering\n",
    "for each (Entity + Context String) in the dataset \n",
    "        if we find a match \n",
    "            add it to the Map \n",
    "        else \n",
    "            create new entry in the Map \n",
    "        \n",
    "#### Post Processing \n",
    "Select the the entity representative of each bucket\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Clustering entities in the Naive Approach\n",
    "\n",
    "#### 1. Levenshtein \n",
    "#### 2. Affine Gap Distance \n",
    "#### 3. Jaroâ€“Winkler distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUD DAD\n",
      "VAILL BRILL\n",
      "VAILL HILL\n",
      "VAILL BILL\n",
      "DAVIS DAISY\n",
      "BRILL VAILL\n",
      "BRILL HILL\n",
      "BRILL BILL\n",
      "JONES NUNES\n",
      "WILLIAMS WILLIAM\n",
      "STEVEMNUCHIN STEVENMNUCHIN\n",
      "FOX ROE\n",
      "FOX COX\n",
      "HOYER DYER\n",
      "HOYER HAYES\n",
      "CORMAN HERMAN\n",
      "CORMAN JORDAN\n",
      "CORMAN MORGAN\n",
      "CORMAN HOMAN\n",
      "DAD JUD\n",
      "DAD WADE\n",
      "LEAR LEARS\n",
      "LEAR AZAR\n",
      "LEAR LARA\n",
      "VIC TIM\n",
      "VIC TIA\n",
      "VIC ERIC\n",
      "LEARS LEAR\n",
      "LEARS LARA\n",
      "REGAN MEGHAN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-1d28971f8ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevenshtein\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e4e63f501c3b>\u001b[0m in \u001b[0;36mlevenshtein\u001b[0;34m(seq1, seq2)\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 )\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     print (matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict = {} \n",
    "for word in entity_people_names_CAPITALS:\n",
    "    if dict.get(word) != None:\n",
    "        dict[word] += 1\n",
    "    else:\n",
    "        dict[word] = 1\n",
    "        \n",
    "# print(dict.items())\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(list(dict.items()), columns = ['column1','column2']) \n",
    "# df.head(20)\n",
    "\n",
    "for word1 in df['column1']:\n",
    "    for word2 in df['column1']:\n",
    "        value = levenshtein(word1, word2)\n",
    "        if (value > 0 and value < 3):\n",
    "            print(word1 + \" \" + word2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 1 - Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "#     print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"Hello\", \"Bebbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature 2 - Affine Gap Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
